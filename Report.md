Credit Card Fraud Detection Analysis Report
1. Executive Summary
The objective of this analysis was to develop a predictive model capable of identifying fraudulent credit card transactions. Given the extreme class imbalance (where fraud represents approximately 0.17% of transactions), the project utilized the SMOTE oversampling technique and compared a linear baseline (Logistic Regression) against a boosting ensemble (XGBoost). The final pipeline prioritizes Recall to minimize the financial risk of undetected fraud.
2. Methodology 2.1 Data Preprocessing & EDA Duplicate Handling: Removed duplicate entries to prevent model over-optimization on repeated samples.Distribution Analysis: EDA revealed that fraud transactions do not follow the same time distribution as legitimate ones, suggesting "Time" is a relevant feature.Outlier Detection: Boxplots were generated for features \(V11\), \(V2\), \(V17\), \(V4\), and \(Amount\) to identify extreme variances.
3. 2.2 Handling Class Imbalance
To prevent the model from defaulting to a "Never Fraud" prediction (which would yield high accuracy but zero utility), the following steps were taken:
Stratified Splitting: Maintained class ratios in the 80/20 train-test split.
SMOTE: Applied Synthetic Minority Over-sampling to the training set to create a 50/50 balanced class distribution for the learner.
3. Model Performance Comparison
Metric	Logistic Regression (Test)	XGBoost Classifier (Test)
Accuracy	High (~97-98%)	Very High (>99%)
Recall (Fraud)	Excellent	Superior
Precision (Fraud)	Moderate	High
Robustness	Sensitive to outliers	Highly resilient via L1/L2 Reg
3.1 XGBoost Optimization
The XGBoost model was configured with specific hyperparameters to handle the synthetic data generated by SMOTE:
max_depth=4 & min_child_weight=3: Prevents the model from growing too deep and memorizing noise.
reg_alpha=2 & reg_lambda=2: Applies L1 and L2 regularization to penalize complexity.
subsample=0.7: Improves generalization by training on subsets of the data.
4. Key Findings & Recommendations Metric Selection: Accuracy is a "vanity metric" in this context. The Recall Score and Precision-Recall Curve are the primary indicators of success.Fraud Patterns: The correlation matrix identified specific \(V\) components (PCA-transformed features) that have strong positive or negative correlations with the Class variable, serving as the strongest predictors.Future Work:Implement RobustScaler to handle the outliers identified in the EDA phase.Deploy an Isolation Forest as an unsupervised alternative to catch anomalous patterns not present in the training set.
5. 5. Implementation Notes
The project was developed in a Google Colab environment using scikit-learn, xgboost, and imblearn. For production deployment, it is recommended to use the Imbalanced-Learn Pipeline to ensure SMOTE is only applied during the training folds of cross-validation
